{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text label\n",
      "0  해운대룸싸롱 해운대룸바 노래주점 해운대가라오케 그렇게 ★즐 겨용 해운대고구려룸 지금...    유해\n",
      "1  메타랩스, -4.15% VI 발동메타랩스(090370)가 VI가 발동했다. 전일 대...   부동산\n",
      "2  베비언스 핑크퐁 유아세제 리필형, 2200ml, 3개향도 좋고 늘 애용하는 제품입니...    진성\n",
      "3  [아모레 헤어바디 공식몰] 미쟝센 헬로크림 염색약 125g초보라서 꼼꼼하게 바르지 ...    진성\n",
      "4  정회원 신청합니다************* 신청 양식 ************ ◎ 네이...    광고\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "#데이터 불러오기 \r\n",
    "T_data = pd.read_excel('all_real_data.xlsx')\r\n",
    "G_data = pd.read_excel('all_garbage_data.xlsx')\r\n",
    "\r\n",
    "T_data = T_data[:30000]\r\n",
    "\r\n",
    "data_df = pd.concat([T_data, G_data])\r\n",
    "\r\n",
    "T_data = None\r\n",
    "G_data = None\r\n",
    "\r\n",
    "#행 섞기 \r\n",
    "data_df = data_df.sample(frac=1).reset_index(drop=True)\r\n",
    "print(data_df.head())\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#라벨인코더를 사용하여 라벨 인코딩\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "label_encoder = LabelEncoder()\r\n",
    "data_df['label'] = label_encoder.fit_transform(data_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>",
      "text/plain": "Empty DataFrame\nColumns: [text, label]\nIndex: []"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#전처리\r\n",
    "##빈 text -> NaN, 중복제거, NaN 행 제거 \r\n",
    "data_df['text'] = data_df['text'].str.replace(\"[^가-힣 ]\",\"\")\r\n",
    "# data_df['text'] = data_df['text'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "data_df['text'].replace('', np.nan, inplace=True)\r\n",
    "data_df = data_df.dropna(how='any')\r\n",
    "data_df.drop_duplicates(['text'], inplace=True)\r\n",
    "data_df.loc[data_df.text.isnull()]\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a785c9deb853>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmecab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMecab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdata_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmecab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmorphs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokenized'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4198\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4199\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4200\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4202\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\eunjeon\\_mecab.py\u001b[0m in \u001b[0;36mmorphs\u001b[1;34m(self, phrase)\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;34m\"\"\"Parse phrase to morphemes.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnouns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\eunjeon\\_mecab.py\u001b[0m in \u001b[0;36mpos\u001b[1;34m(self, phrase, flatten)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mflatten\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from eunjeon import Mecab\r\n",
    "mecab = Mecab()\r\n",
    "\r\n",
    "data_df['text'] = data_df['text'].apply(mecab.morphs)\r\n",
    "data_df['tokenized'] = data_df['text'].apply(lambda x : [item for item in x if len(item) > 1 ])\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_df['tokenized']\r\n",
    "y = data_df['label']\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.head())\r\n",
    "print(y_train.head())\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\r\n",
    "    if x == []:\r\n",
    "        return ''\r\n",
    "    else:\r\n",
    "        return x \r\n",
    "\r\n",
    "data_df['tokenized'] = data_df['tokenized'].apply(lambda x :f(x))\r\n",
    "data_df['tokenized'].replace('', np.nan, inplace=True)\r\n",
    "data_df = data_df.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data_df.index))\r\n",
    "# data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer\r\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
    "\r\n",
    "tokenizer = Tokenizer()\r\n",
    "\r\n",
    "tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.word_index)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 14\r\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\r\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\r\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\r\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\r\n",
    "\r\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\r\n",
    "for key, value in tokenizer.word_counts.items():\r\n",
    "    total_freq = total_freq + value\r\n",
    "\r\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\r\n",
    "    if(value < threshold):\r\n",
    "        rare_cnt = rare_cnt + 1\r\n",
    "        rare_freq = rare_freq + value\r\n",
    "\r\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\r\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\r\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\r\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) -rare_cnt\r\n",
    "# vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab_size, oov_token='OOV')\r\n",
    "\r\n",
    "tokenizer.fit_on_texts(x_train)\r\n",
    "\r\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\r\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train = [index for index, sentence in enumerate(x_train) if len(sentence) < 1]\r\n",
    "drop_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.delete(x_train, drop_train, axis=0)\r\n",
    "y_train = np.delete(y_train, drop_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "print('리뷰의 최대 길이 :',max(len(l) for l in x_train))\r\n",
    "print('리뷰의 평균 길이 :',sum(map(len, x_train))/len(x_train))\r\n",
    "plt.hist([len(s) for s in x_train], bins=50)\r\n",
    "plt.xlabel('length of samples')\r\n",
    "plt.ylabel('number of samples')\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\r\n",
    "  cnt = 0\r\n",
    "  for s in nested_list:\r\n",
    "    if(len(s) <= max_len):\r\n",
    "        cnt = cnt + 1\r\n",
    "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))\r\n",
    "\r\n",
    "max_len = 600\r\n",
    "below_threshold_len(max_len, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
    "\r\n",
    "x_train = pad_sequences(x_train, maxlen = max_len)\r\n",
    "x_test = pad_sequences(x_test, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = np.max(y_train)+1\r\n",
    "print(\"카테고리의 종류 : \",category)\r\n",
    "print(x_train.shape)\r\n",
    "print(y_train.shape)\r\n",
    "print(np.unique(y_train))\r\n",
    "\r\n",
    "from tensorflow.keras.utils import to_categorical\r\n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Conv1D,Dropout, Conv1D, GlobalMaxPooling1D, Dense, BatchNormalization,Input, concatenate\r\n",
    "from tensorflow.keras.models import Sequential, Model\r\n",
    "from tensorflow.keras.models import load_model\r\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\r\n",
    "\r\n",
    "input1 = Input(shape=(None,), dtype='int32')\r\n",
    "model1 = Embedding(vocab_size+2, 512)(input1)\r\n",
    "model1 = Dropout(0.3)(model1)\r\n",
    "model1 = Conv1D(512, 2, padding='valid', activation='relu')(model1)\r\n",
    "model1 = GlobalMaxPooling1D()(model1)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# input2 = Input(shape=(None,), dtype='int32')\r\n",
    "# model2 = Embedding(vocab_size+2, 512)(input2)\r\n",
    "# model2 = Dropout(0.3)(model2)\r\n",
    "# model2 = Conv1D(512, 4, padding='valid', activation='relu')(model2)\r\n",
    "# model2 =GlobalMaxPooling1D()(model2)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "input3 = Input(shape=(None,), dtype='int32')\r\n",
    "model3 = Embedding(vocab_size+2, 512)(input3)\r\n",
    "model3 = Dropout(0.3)(model3)\r\n",
    "model3 = Conv1D(512, 6, padding='valid', activation='relu')(model3)\r\n",
    "model3 = GlobalMaxPooling1D()(model3)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "concatenate = concatenate([model1, model3])\r\n",
    "# concatenate = concatenate([model1, model2, model3])\r\n",
    "\r\n",
    "# concatenate = concatenate([model1])\r\n",
    "\r\n",
    "\r\n",
    "concatenate = Dense(128, activation='relu')(concatenate)\r\n",
    "output = Dense(6, activation='softmax')(concatenate)\r\n",
    "\r\n",
    "model = Model([input1, input2, input3], output)\r\n",
    "\r\n",
    "\r\n",
    "# model=Sequential()\r\n",
    "# model.add(Embedding(vocab_size+2, 512))\r\n",
    "# model.add(Dropout(0.3))\r\n",
    "# model.add(Conv1D(512, 2, padding='valid', activation='relu'))\r\n",
    "# model.add(GlobalMaxPooling1D())\r\n",
    "# model.add(Dense(128, activation='relu'))\r\n",
    "# model.add(Dense(6, activation='softmax'))\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import Embedding, Dense, LSTM, Conv1D,Dropout, Conv1D, GlobalMaxPooling1D, Dense, BatchNormalization,Input, concatenate\r\n",
    "# from tensorflow.keras.models import Sequential, Model\r\n",
    "# from tensorflow.keras.models import load_model\r\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\r\n",
    "\r\n",
    "# input1 = Input(shape=(None,))\r\n",
    "# model1 = Embedding(vocab_size+2, 512)(input1)\r\n",
    "# model1 = Dropout(0.3)(model1)\r\n",
    "# model1 = Conv1D(512, 2, padding='valid', activation='relu')(model1)\r\n",
    "# model1 = GlobalMaxPooling1D()(model1)\r\n",
    "\r\n",
    "# output = Dense(6, activation='softmax')(model1)\r\n",
    "\r\n",
    "# model = Model(input1, output)\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\r\n",
    "start = time.time()\r\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\r\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\r\n",
    "\r\n",
    "model.fit([x_train ,x_train], y_train, epochs=100, callbacks=[es], batch_size=60, validation_split=0.2)\r\n",
    "# model.fit([x_train, x_train ,x_train], y_train, epochs=100, callbacks=[es], batch_size=60, validation_split=0.2)\r\n",
    "\r\n",
    "# mode = load_model('saved/conv1D_model.h5')\r\n",
    "end = time.time()\r\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\r\n",
    "model.save('saved/conv1D_ensemble_model.h5')\r\n",
    "pickle.dump(tokenizer,open('saved/conv1D_ensemble_tokenizer.pkl','wb'))\r\n",
    "pickle.dump(label_encoder,open('saved/conv1D_ensemble_labelencoder.pkl','wb'))\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "acc = accuracy_score(y_test.values, y_predict)\r\n",
    "print(acc)\r\n",
    "print(label_encoder.inverse_transform(y_predict[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit (conda)",
   "name": "python385jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
