# 딥러닝

## 딥러닝과 머신러닝의 차이



머신러닝은 딥러닝을 포함한 개념

- 머신러닝
  - 데이터로부터 패턴을 분석하여 예측하는 방법
  - 사람이 직접 특징을 추출한 정보를 기반으로 학습
  - 상대적으로 적은 데이터로 좋은 성능을 낼 수 있음
- 딥러닝
  - 분류를 통해 결과를 예측하는 것
  - 컴퓨터가 스스로 중요한 특징을 추출하여 학습
  - 컴퓨터가 합당한 특징을 추출 할 수 있도록 상대적으로 많은 데이터가 필요



<br>

## 활성함수란?

입력받은 데이터를 적절하게 처리하여 출력하는 함수

- sigmoid

  - 항상 0과 1 **사이** 의 값만 출력하는 함수
  - Step 함수는 0 혹은 1 중간값이 없다.
  - 이진분류 시 사용되는 함수

- Relu

  - 선형함수로  r <= 0 라면 0으로 r >= 0  r로 출력하는 함수
  - Gradient Vanishing을 해결하기 위해 많이 사용 되는 함수
  - 유사한 함수로 Selu, Elu 등이 존재
    - 유사한 함수들은 0이하의 값들이 소실 되는 것을 방지하기위해 0이 아닌 0에 가까운 작은 값으로 반환

- SoftMax

  - 다중분류 문제를 풀 때 많이 사용되는 함수

  - 모든 출력의 합이 1이 되게 만들어 출력

   



<br>

## 손실함수란?(loss function, Cost Function)

입력을 받아 훈련된 가중치(w)와 바이어스(b)를 사용하여 기대출력을 만들어내고 실제 출력과 기대출력간의 차

이러한 손실함수의 결과를 사용하여 W,B를 수정한다.

- MSE
  - 예측값과 실제 값 사이의 평균 제곱오차를 구함
  - 제곱연산으로 오차가 크다면 더욱 뚜렷하게 나타남
  - 제곱으로 인해 오차가 양수, 음수 모두 양수로 표현됨
- RMSE
  - 예측값과 실제값의 평균제곱오차에 루트를 씌운것
  - MSE는 제곱 연산으로 인해 실제 오류보다 값이 커져 왜곡이 커짐
  - RMSE는 루트의 사용으로 왜곡이 줄어듬
- Binary Crossentropy
  - 실제 레이블과 예측 레이블간 교차 엔트로피 손실을 계산
  - 주로 이진 분류 시 사용
- Categorical Crossentropy
  - 실제 레이블과 예측 레이블간 교차 엔트로피 손실을 계산
  - 주로 다중분 시 사용



<br>



## 경사하강법이란?(Gradient-decent)

좁은 범위에서 기울기가 가장 큰 방향으로 그래프를 내려가는 방법으로 손실함수가 최소값이 되는 지점을 찾아가는 방법

- Learning Rate (학습 조절 변수)
  - 경사 하강 시 사용되는 변수로 그래프에서 다음 위치의 영향을 줌
  - 높은 LR는  값이 발산하게 됨
  - 낮은 LR은 손실함수가 0으로 가는 시간을 매우 느리게 함
  - 원하는 위치보다 높은 위치에서 멈출 수 있다 Local minima

<img height=400 src='https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-08-16-gradient_descent/pic5.png'>

<br>

## Back-propagation

입력과 출력을 알고 있는 상태에서신경망을 학습시키는 방법

오차를 각각 가중치로편미분한다.

출력에서 입력의 방향으로 진행한디.

실질적으로 w와 b를 수정하는 것은 역전파(back propagation)



<br>



## Gradient Vanishing이란?

역전파가 이루어지며 출력에서 입력으로 오차를 가중치로 편미분 하는과정에서 입력방향으로 갈 수록 가중치가 0으로 되거가는 상황

- Exploding
  - LR이 너무  높게 되면 움직임이 매우 커지게 되어 발산한다.

<br>



## overffit과 underffit

공통점: 예측성능 저하

- Overffiting
  - 학습 데이터에 대해 과적합 되어있는 상태로 새로운 데이터에 대한 유연한 예측이 불가
  - 너무 많은 feature를 사용 할 시 발생
  - 많은 epoch(학습)을 진행할 시 발생
  - 편중된 학습데이터
  - Dropout사용(Reguliazation 효과 발생)
- Underffiting
  - 충분한 학습이 진행되지 않아 유효한 예측이 불가
  - 너무 적은 feature사용으로 유효한 특징, 패턴을 찾을 수 없는 상황
  - 충분치 못한 데이터

<br>

## Dropout

노드의 일부를 생략하는 것으로 Reguliazation효과를 얻는다.

높은성능을 위해서는 서로 다른 데이터를 학습시키거나 서로 다른 신경망을 사용해야함

복수의 신경망을 구성하는 것을 대신하여 신경망의 노드를 무작위로 offine으로 두어 진행하여 다양한 신경망을 학습시키는 것으로 대체 가능

<br>

## Nomalization

<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile21.uf.tistory.com%2Fimage%2F9918EB3C5E13E36F3B75B8'>

unnomalized의경우 w,b를 설정 시 최악의 위치에서 시작되는 경우 많은 학습시간, local min에 빠질 수 있다.

nomalized는 이러한 그래프를 평탄하게 만들어주어 어느 위치에서 w,b가 시작하더라도 비슷한 속도로 손실함수가 0인 지점으로 도달하도록 한다. w,b위치에 따라 LR이 달라지는 상황도 다소 방지해준다.(?) Dropout, BatchNomalization의 사용으로 이러한 효과를 얻을 수 있다.

<br>

## 가중치초기화

가중치의 값이 모두 같게 되면 모두 같은 출력값을 가지게 될 것이고 역전파 단계에서 모두 같은 Gradient 가지게 된다.

모두 같은 Gradient를 가지게되면 1개의 뉴런으로 동작하기 때문에 서로 다른 초기의 가중치를 가지도록 설정 해야한다.

- he initialization
  - Relu계열과 함께 사용할시 좋은 성능을 나타냄

- Xabier Initialization
  - sigmoid, tanh 활성 함수 사용시 좋은 성능을 나타냄



- unifrom
  - 최소값과 최대값 사이의 모두 같은 확률로 추출
- nomal
  - 가우시안 분포의 그래프에서 평균에 가까울 수록 높은 확률로 추출되며 평균에서 멀어질 수록 낮은 확률로 추출





<br>

## Optimizer

손실함수가 0인 지점을 찾기위해 가중치, 바이어스를 조정하는 방식을 지정

- SGD 확률적 경사하강법
  - 가장 기초적인 경사하강법으로 기울기의 손실함수의 기울기가 기울어진 방향으로 일정하게 이동
- 모멘텀
  - 이전의 이동값을 일정 비율 반영시켜 관성의 효과를 내어 기울어진 방향으로 이동
- AdaGrad
  - 학습률을 변동시켜 이동하는 방식, 크게 변화하는 변수는 학습률을 작게, 작게 변화하는 변수는 학습률을 크게 조정한다.
- Adam
  - 학습률 변동과 모멘텀을 합친 것

Todo








CNN

DNN

RNN

------------------------------

# 머신러닝